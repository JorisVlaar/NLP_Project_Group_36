{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d185759e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joris\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random as r\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import tqdm\n",
    "from sklearn.base import clone\n",
    "import ast\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da36c090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>['action', 'first-person', 'shooter', 'multipl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>['2d', 'fighting', 'martial arts', 'intentiona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100400</td>\n",
       "      <td>['animation &amp; modeling', 'software']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10090</td>\n",
       "      <td>['zombies', 'world war ii', 'first-person', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100980</td>\n",
       "      <td>['animation &amp; modeling', 'utilities', 'design ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10100</td>\n",
       "      <td>['adventure', 'point &amp; click', 'classic', 'fan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10110</td>\n",
       "      <td>['adventure', 'point &amp; click', 'classic', 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10120</td>\n",
       "      <td>['action', 'casual', 'space', 'on-rails shoote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10130</td>\n",
       "      <td>['action', 'first-person', 'shooter', 'time ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10140</td>\n",
       "      <td>['sports', 'mini golf', 'golf', 'local multipl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   app_id                                               tags\n",
       "0      10  ['action', 'first-person', 'shooter', 'multipl...\n",
       "1    1002  ['2d', 'fighting', 'martial arts', 'intentiona...\n",
       "2  100400               ['animation & modeling', 'software']\n",
       "3   10090  ['zombies', 'world war ii', 'first-person', 's...\n",
       "4  100980  ['animation & modeling', 'utilities', 'design ...\n",
       "5   10100  ['adventure', 'point & click', 'classic', 'fan...\n",
       "6   10110  ['adventure', 'point & click', 'classic', 'com...\n",
       "7   10120  ['action', 'casual', 'space', 'on-rails shoote...\n",
       "8   10130  ['action', 'first-person', 'shooter', 'time ma...\n",
       "9   10140  ['sports', 'mini golf', 'golf', 'local multipl..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1. Load your datasets\n",
    "# -----------------------------\n",
    "print(\"Loading datasets...\")\n",
    "reviews_df = pd.read_csv(\"./archive/dataset.csv\")  # columns: app_id, app_name, review_text\n",
    "print(reviews_df.head(3))\n",
    "\n",
    "tags_df = pd.read_csv(\"comp/tagGamesNoCompoundDF.csv\")\n",
    "tagCountsDF = pd.read_csv(\"comp/tagCountsDF.csv\")\n",
    "tags_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f14bbb69",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sentence_transformers\\SentenceTransformer.py:1621\u001b[39m, in \u001b[36mSentenceTransformer.tokenize\u001b[39m\u001b[34m(self, texts, **kwargs)\u001b[39m\n\u001b[32m   1620\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sentence_transformers\\models\\Transformer.py:307\u001b[39m, in \u001b[36mTransformer.tokenize\u001b[39m\u001b[34m(self, texts, padding)\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text_tuple \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     batch1.append(\u001b[43mtext_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m    308\u001b[39m     batch2.append(text_tuple[\u001b[32m1\u001b[39m])\n",
      "\u001b[31mTypeError\u001b[39m: 'float' object is not subscriptable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 99\u001b[39m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m allowed_reviews\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# Example Usage\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# Step 1: Compute and store similarities per app_id\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m similarity_results = \u001b[43mcompute_similarity_per_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreviews_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Step 2: Filter later by threshold\u001b[39;00m\n\u001b[32m    102\u001b[39m allowed_reviews = filter_reviews_by_threshold(similarity_results, threshold=\u001b[32m0.5\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mcompute_similarity_per_app\u001b[39m\u001b[34m(reviews_df, tags_df, batch_size)\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     60\u001b[39m review_texts = group[\u001b[33m'\u001b[39m\u001b[33mreview_text\u001b[39m\u001b[33m'\u001b[39m].tolist()\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m review_embeddings = \u001b[43mencode_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Compute cosine similarity (reviews x tags)\u001b[39;00m\n\u001b[32m     64\u001b[39m sim_matrix = cosine_similarity(review_embeddings, tag_embeddings)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mencode_in_batches\u001b[39m\u001b[34m(texts, batch_size, model)\u001b[39m\n\u001b[32m     19\u001b[39m     end = start + batch_size\n\u001b[32m     20\u001b[39m     batch = texts[start:end]\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     emb_batch = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     embeddings.append(emb_batch)\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.vstack(embeddings)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sentence_transformers\\SentenceTransformer.py:1062\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[39m\n\u001b[32m   1060\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m start_index \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), batch_size, desc=\u001b[33m\"\u001b[39m\u001b[33mBatches\u001b[39m\u001b[33m\"\u001b[39m, disable=\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[32m   1061\u001b[39m     sentences_batch = sentences_sorted[start_index : start_index + batch_size]\n\u001b[32m-> \u001b[39m\u001b[32m1062\u001b[39m     features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1063\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1064\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sentence_transformers\\SentenceTransformer.py:1623\u001b[39m, in \u001b[36mSentenceTransformer.tokenize\u001b[39m\u001b[34m(self, texts, **kwargs)\u001b[39m\n\u001b[32m   1621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[\u001b[32m0\u001b[39m].tokenize(texts, **kwargs)\n\u001b[32m   1622\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sentence_transformers\\models\\Transformer.py:307\u001b[39m, in \u001b[36mTransformer.tokenize\u001b[39m\u001b[34m(self, texts, padding)\u001b[39m\n\u001b[32m    305\u001b[39m batch1, batch2 = [], []\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text_tuple \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     batch1.append(\u001b[43mtext_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m    308\u001b[39m     batch2.append(text_tuple[\u001b[32m1\u001b[39m])\n\u001b[32m    309\u001b[39m to_tokenize = [batch1, batch2]\n",
      "\u001b[31mTypeError\u001b[39m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# -------------------------------\n",
    "# Initialize model\n",
    "# -------------------------------\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# -------------------------------\n",
    "# Helper: encode in batches\n",
    "# -------------------------------\n",
    "def encode_in_batches(texts, batch_size=512, model=model):\n",
    "    embeddings = []\n",
    "    for start in range(0, len(texts), batch_size):\n",
    "        end = start + batch_size\n",
    "        batch = texts[start:end]\n",
    "        emb_batch = model.encode(batch, show_progress_bar=False)\n",
    "        embeddings.append(emb_batch)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# -------------------------------\n",
    "# Helper: save intermediate results\n",
    "# -------------------------------\n",
    "def save_intermediate(app_id, data, folder='intermediate_results'):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    file_path = os.path.join(folder, f'{app_id}.pkl')\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "# -------------------------------\n",
    "# Helper: load intermediate if exists\n",
    "# -------------------------------\n",
    "def load_intermediate(app_id, folder='intermediate_results'):\n",
    "    file_path = os.path.join(folder, f'{app_id}.pkl')\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "# -------------------------------\n",
    "# Compute similarities per app_id in batches\n",
    "# -------------------------------\n",
    "def compute_similarity_per_app(reviews_df, tags_df, batch_size=512):\n",
    "    # Encode all tags once\n",
    "    tag_embeddings = encode_in_batches(tags_df['tags'].tolist(), batch_size=batch_size)\n",
    "\n",
    "    results = {}  # store similarities per app_id\n",
    "\n",
    "    for app_id, group in reviews_df.groupby('app_id'):\n",
    "        # Try to load intermediate result\n",
    "        data = load_intermediate(app_id)\n",
    "        if data is not None:\n",
    "            results[app_id] = data\n",
    "            continue\n",
    "\n",
    "        review_texts = group['review_text'].tolist()\n",
    "        review_embeddings = encode_in_batches(review_texts, batch_size=batch_size)\n",
    "\n",
    "        # Compute cosine similarity (reviews x tags)\n",
    "        sim_matrix = cosine_similarity(review_embeddings, tag_embeddings)\n",
    "\n",
    "        # Store full similarity matrix for later threshold filtering\n",
    "        results[app_id] = {\n",
    "            'reviews': review_texts,\n",
    "            'similarity_matrix': sim_matrix\n",
    "        }\n",
    "\n",
    "        # Save intermediate result per app_id\n",
    "        save_intermediate(app_id, results[app_id])\n",
    "\n",
    "    return results\n",
    "\n",
    "# -------------------------------\n",
    "# Filter reviews by threshold (any tag)\n",
    "# -------------------------------\n",
    "def filter_reviews_by_threshold(similarity_results, threshold=0.5):\n",
    "    allowed_reviews = []\n",
    "    for app_id, data in similarity_results.items():\n",
    "        sim_matrix = data['similarity_matrix']\n",
    "        review_texts = data['reviews']\n",
    "\n",
    "        # Boolean mask: any tag above threshold\n",
    "        mask = np.any(sim_matrix > threshold, axis=1)\n",
    "\n",
    "        for review, keep in zip(review_texts, mask):\n",
    "            if keep:\n",
    "                allowed_reviews.append((app_id, review))\n",
    "\n",
    "    return allowed_reviews\n",
    "\n",
    "# -------------------------------\n",
    "# Example Usage\n",
    "# -------------------------------\n",
    "# Step 1: Compute and store similarities per app_id\n",
    "similarity_results = compute_similarity_per_app(reviews_df, tags_df, batch_size=512)\n",
    "\n",
    "# Step 2: Filter later by threshold\n",
    "allowed_reviews = filter_reviews_by_threshold(similarity_results, threshold=0.5)\n",
    "allowed_reviews_df = pd.DataFrame(allowed_reviews, columns=['app_id', 'review_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d857fcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_removed_reviews(original_df, allowed_df):\n",
    "    # Count total reviews per app_id\n",
    "    total_counts = original_df.groupby('app_id').size().rename('total_reviews')\n",
    "    \n",
    "    # Count allowed reviews per app_id\n",
    "    allowed_counts = allowed_df.groupby('app_id').size().rename('allowed_reviews')\n",
    "    \n",
    "    # Combine counts\n",
    "    comparison = pd.concat([total_counts, allowed_counts], axis=1).fillna(0)\n",
    "    \n",
    "    # Compute removed reviews\n",
    "    comparison['removed_reviews'] = comparison['total_reviews'] - comparison['allowed_reviews']\n",
    "    \n",
    "    return comparison.reset_index()\n",
    "\n",
    "# Example usage:\n",
    "removed_summary = count_removed_reviews(reviews_df, allowed_reviews)\n",
    "print(removed_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c3ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2. Aggregate reviews per game (only games with tags available)\n",
    "# -----------------------------\n",
    "\n",
    "def aggregate_reviews_per_game(reviews_df: pd.DataFrame, tags_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(\"Aggregating reviews per game...\")\n",
    "\n",
    "    # Standardize key column name\n",
    "    reviews_df[\"app_id\"] = reviews_df[\"app_id\"].astype(int, copy=False)\n",
    "    tags_df[\"app_id\"] = tags_df[\"app_id\"].astype(int, copy=False)\n",
    "\n",
    "    # Filter reviews that have tags\n",
    "    reviews_with_tags = (\n",
    "        reviews_df.loc[reviews_df['app_id'].isin(tags_df['app_id']), ['app_id', 'app_name', 'review_text']]\n",
    "        .assign(review_text=lambda df: df['review_text'].astype(str))\n",
    "    )\n",
    "\n",
    "    # Filter short reviews\n",
    "    reviews_with_tags['review_text'] = reviews_with_tags['review_text'].astype(str)\n",
    "    reviews_with_tags = reviews_with_tags.loc[\n",
    "        reviews_with_tags['review_text'].str.count(r'\\S+') >= 3\n",
    "    ]\n",
    "    print(f\"Filtered reviews shape: {reviews_with_tags.shape}\")\n",
    "\n",
    "    # Aggregate per game\n",
    "    game_reviews = (\n",
    "        reviews_with_tags\n",
    "        .groupby(\"app_id\", as_index=False)\n",
    "        .agg({\n",
    "            'review_text': lambda texts: \" \".join(texts),\n",
    "            'app_name': 'first'\n",
    "        })\n",
    "    )\n",
    "    print(f\"Aggregated reviews shape: {game_reviews.shape}\")\n",
    "\n",
    "    # Merge and parse tags\n",
    "    game_reviews = game_reviews.merge(tags_df, on=\"app_id\", how='inner')\n",
    "    game_reviews['tags'] = game_reviews['tags'].apply(ast.literal_eval)\n",
    "\n",
    "    print(f\"Merged game_reviews shape: {game_reviews.shape}\")\n",
    "    print(\"Sample of final aggregated data:\")\n",
    "    print(game_reviews.head(3))\n",
    "\n",
    "    return game_reviews\n",
    "\n",
    "game_reviews = aggregate_reviews_per_game(reviews_df, tags_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
